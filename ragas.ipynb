{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6343f7be-df74-48f9-a99c-a600845efacf",
   "metadata": {},
   "source": [
    "## 🧠 What is RAGAS?\n",
    "RAGAS (Retrieval-Augmented Generation Assessment Suite) is an open-source evaluation framework for measuring the performance and quality of RAG (Retrieval-Augmented Generation) systems. It allows developers to assess how well a RAG pipeline retrieves relevant information and generates accurate, context-aware responses.\n",
    "\n",
    "## 📌 Key Features\n",
    " * Automatic Evaluation using LLMs (e.g., OpenAI GPT)\n",
    " * Multiple Metrics for both retriever and generator components\n",
    " * Supports Custom Datasets\n",
    " * Integration-friendly with LangChain, LlamaIndex, and other RAG frameworks\n",
    "\n",
    "## 🔍 Why RAGAS?\n",
    "Traditional evaluation methods like ROUGE or BLEU are not suitable for open-ended or generative tasks. RAGAS leverages large language models to act as evaluators, mimicking human judgment while maintaining scalability and consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cf463-619b-4f3c-892a-0cc43141fa39",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "### 1. Navigate to the Project Directory\n",
    "cd llm-sandbox\n",
    "\n",
    "### 2. Create a Virtual Environment\n",
    "python3 -m venv llm-env\n",
    "\n",
    "### 3. Activate the Virtual Environment\n",
    "source llm-env/bin/activate\n",
    "\n",
    "### 4. Install Required Libraries\n",
    "pip install ragas datasets openai pillow ipywidgets notebook ipykernel\n",
    "\n",
    "### 5. Register the Virtual Environment as a Jupyter Kernel\n",
    "python -m ipykernel install --user --name=llm-env --display-name \"Python (llm-env)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21850db0-6f19-43ca-bb7a-0a3c8d1be594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os  # For environment variable management\n",
    "import getpass  # For secure input of API keys\n",
    "\n",
    "# Import dataset utility from Hugging Face\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import main evaluation function from ragas\n",
    "from ragas import evaluate\n",
    "\n",
    "# Import evaluation metrics from ragas\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c586de-fad5-4800-9a79-df7caa48598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "api_key = getpass.getpass(\"Please input OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1233f81-3a4a-46ec-8d3f-c1c78d8eba3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is LLM as a Judge?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     ]\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert the dictionary to a Hugging Face Dataset object\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_dict(data)\n\u001b[1;32m     23\u001b[0m dataset\u001b[38;5;241m.\u001b[39mpandas()\u001b[38;5;241m.\u001b[39mhead()  \u001b[38;5;66;03m# Display the first few rows of the dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the evaluation data\n",
    "data = {\n",
    "    \"question\": [\n",
    "        \"What is LLM as a Judge?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"LLM as a Judge is a concept where a large language model is used to evaluate the output of another model. It combines human-like reasoning with machine efficiency.\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        [\n",
    "            \"Evaluating tasks performed by LLMs can be difficult due to their complexity and the diverse criteria involved. Traditional methods like rule-based assessment or similarity metrics (e.g., ROUGE, BLEU) often fall short when applied to the nuanced and varied outputs of LLMs.\",\n",
    "            \"For instance, an AI assistant’s answer to a question can be: not grounded in context, repetitive, grammatically incorrect, excessively lengthy, incoherent. The list of criteria goes on. And even if we had a limited list, each of these would be hard to measure.\",\n",
    "            \"To overcome this challenge, the concept of \\\"LLM as a Judge\\\" employs an LLM to evaluate another's output, combining human-like assessment with machine efficiency.\"\n",
    "        ]\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"The concept of \\\"LLM as a Judge\\\" employs an LLM to evaluate another's output, combining human-like assessment with machine efficiency.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a Hugging Face Dataset object\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset.pandas().head()  # Display the first few rows of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77ddcdad-4b7b-4543-9ba0-cd518537d85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████| 4/4 [00:09<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(dataset, metrics=[\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59020ab5-c110-48c6-afac-2c944bbbcef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== RAGAS Evaluation Results =====\n",
      "faithfulness: 1.000\n",
      "answer_correctness: 0.838\n",
      "context_precision: 0.333\n",
      "context_recall: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Then use these metrics in evaluation\n",
    "print(\"===== RAGAS Evaluation Results =====\")\n",
    "for metric, score in results.scores[0].items():\n",
    "    print(f\"{metric}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "147b08cc-6c79-4b9b-8104-4979df7002c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is LLM as a Judge?</td>\n",
       "      <td>[Evaluating tasks performed by LLMs can be dif...</td>\n",
       "      <td>LLM as a Judge is a concept where a large lang...</td>\n",
       "      <td>The concept of \"LLM as a Judge\" employs an LLM...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.837517</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user_input                                 retrieved_contexts  \\\n",
       "0  What is LLM as a Judge?  [Evaluating tasks performed by LLMs can be dif...   \n",
       "\n",
       "                                            response  \\\n",
       "0  LLM as a Judge is a concept where a large lang...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  The concept of \"LLM as a Judge\" employs an LLM...           1.0   \n",
       "\n",
       "   answer_correctness  context_precision  context_recall  \n",
       "0            0.837517           0.333333             1.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
